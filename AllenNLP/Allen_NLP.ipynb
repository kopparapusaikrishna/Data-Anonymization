{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8c6IQRIHoPCZ",
        "foAHHabWodPa",
        "oom0Xk2ji7rX",
        "MzPzJ_LjjDRS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Midterm"
      ],
      "metadata": {
        "id": "4Kj6A_Hgz1Ao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of libraries"
      ],
      "metadata": {
        "id": "8c6IQRIHoPCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyeQwrRIzDht",
        "outputId": "15579a26-853a-4704-f146-5e9a2753514b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp==2.1.0\n",
            "  Downloading allennlp-2.1.0-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting allennlp-models==2.1.0\n",
            "  Downloading allennlp_models-2.1.0-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 83.7 MB/s \n",
            "\u001b[?25hCollecting transformers<4.4,>=4.1\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (1.0.2)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 67.3 MB/s \n",
            "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.26.22-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 97.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (9.0.0)\n",
            "Collecting torchvision<0.9.0,>=0.8.1\n",
            "  Downloading torchvision-0.8.2-cp38-cp38-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (3.1.0)\n",
            "Collecting filelock<3.1,>=3.0\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting spacy<3.1,>=2.1.0\n",
            "  Downloading spacy-3.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (0.99)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting torch<1.8.0,>=1.6.0\n",
            "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 5.1 kB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 54.2 MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 71.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (1.7.3)\n",
            "Collecting jsonpickle\n",
            "  Downloading jsonpickle-3.0.0-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (3.6.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from allennlp==2.1.0) (3.7)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting conllu==4.4\n",
            "  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.22\n",
            "  Downloading botocore-1.29.22-py3-none-any.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.22->boto3<2.0,>=1.14->allennlp==2.1.0) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 82.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.22->boto3<2.0,>=1.14->allennlp==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18->allennlp==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18->allennlp==2.1.0) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18->allennlp==2.1.0) (2022.9.24)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.0.7)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (0.10.1)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 81.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.0.9)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 81.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (0.7.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (3.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.1,>=2.1.0->allennlp==2.1.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.1,>=2.1.0->allennlp==2.1.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1,>=2.1.0->allennlp==2.1.0) (4.1.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=1.2->allennlp==2.1.0) (3.19.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision<0.9.0,>=0.8.1->allennlp==2.1.0) (7.1.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 77.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.4,>=4.1->allennlp==2.1.0) (2022.6.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1,>=2.1.0->allennlp==2.1.0) (7.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->allennlp-models==2.1.0) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->allennlp==2.1.0) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==2.1.0) (22.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==2.1.0) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==2.1.0) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->allennlp==2.1.0) (3.1.0)\n",
            "Building wheels for collected packages: overrides, jsonnet, word2number, sacremoses\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=199b53e011bd8b6674039a0117571f33c9484e22ca8f4841b6574b77dbef0dc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp38-cp38-linux_x86_64.whl size=3996243 sha256=e13b879b88b7ce9be2a7929bfdff9a410e7f015870d6ed3f93d1d2e0daffd454\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/ec/56/de861aae102c449ade2378772abbf9eb7e9acfe9a80f3e6036\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=930462951f8f7b70fffab423037d274603d3d2c8b2c7b921714ee324b57fe0ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/f3/5a/d88198fdeb46781ddd7e7f2653061af83e7adb2a076d8886d6\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=30668d2bdc1e84a2cf9cf3140f2bf1355b041b66b32eb6d22b7f63e33cdae6a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built overrides jsonnet word2number sacremoses\n",
            "Installing collected packages: urllib3, jmespath, typer, pydantic, botocore, torch, tokenizers, thinc, sacremoses, s3transfer, filelock, transformers, torchvision, tensorboardX, spacy, sentencepiece, overrides, jsonpickle, jsonnet, boto3, word2number, py-rouge, ftfy, conllu, allennlp, allennlp-models\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "!pip install allennlp==2.1.0 allennlp-models==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp-models"
      ],
      "metadata": {
        "id": "mmaH8kr1z_op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "K1Gu4FLTz_rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.2.0"
      ],
      "metadata": {
        "id": "4e8WrFdCLmSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Starts Here"
      ],
      "metadata": {
        "id": "xFTCXkYgizM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting the drive"
      ],
      "metadata": {
        "id": "foAHHabWodPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_CXDZTWOjx2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "oom0Xk2ji7rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.tagging\n",
        "import gensim"
      ],
      "metadata": {
        "id": "iLF78tvkz_tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Allen NLP coreference model"
      ],
      "metadata": {
        "id": "MzPzJ_LjjDRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n"
      ],
      "metadata": {
        "id": "Tc5mOITOz_v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady.\""
      ],
      "metadata": {
        "id": "0iY5cz4Hz_yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred = predictor.predict(    \n",
        "#          document= text\n",
        "# )"
      ],
      "metadata": {
        "id": "wuJwC9Zuz_0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clusters = pred['clusters']\n",
        "# tokens = pred['document'] "
      ],
      "metadata": {
        "id": "u9mtbTCOz_21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens"
      ],
      "metadata": {
        "id": "zjDaxfxY8W7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clusters"
      ],
      "metadata": {
        "id": "ToadPsYf8aCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = 0\n",
        "# doc = {}\n",
        "# for word in tokens:   \n",
        "#     doc.update({i :  word})   \n",
        "#     i = i+1"
      ],
      "metadata": {
        "id": "EsDpyumf0QTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clus_all = []\n",
        "# cluster = []\n",
        "# temp = \"\"\n",
        "# for i in range(0, len(clusters)):    \n",
        "#   one_cl = clusters[i]    \n",
        "#   for count in range(0, len(one_cl)):           \n",
        "#     obj = one_cl[count] \n",
        "#     for s in range((obj[0]), (obj[1]+1)):            \n",
        "#       temp = temp + \" \" + doc[s]\n",
        "#     cluster.append(temp[1:])\n",
        "#     temp=\"\"\n",
        "\n",
        "#   clus_all.append(cluster)       \n",
        "#   cluster = []     \n",
        "# print (clus_all) #And finally, this shows all coreferences"
      ],
      "metadata": {
        "id": "_mob5Jp90QZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cr_text = predictor.coref_resolved(text)"
      ],
      "metadata": {
        "id": "uWKIHLwmz_5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(cr_text)"
      ],
      "metadata": {
        "id": "myr79D_Ud-Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Allen NLP NER model"
      ],
      "metadata": {
        "id": "2b9ESzHVjO3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_ner = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\")"
      ],
      "metadata": {
        "id": "FQgZyRrYAOAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_ner"
      ],
      "metadata": {
        "id": "L8isCOeML7Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = open(\"/content/drive/MyDrive/Reading elective/Squad dataset/squad_lite.txt\", \"r\")"
      ],
      "metadata": {
        "id": "bl29xoWTZy7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = text_file.read()\n",
        "data = \"Albert Einstein was born in Germany and lived in England. He was a employee in Google.\"\n",
        "# data = 'Who did Beyonce star with in the movie, \"Austin Powers in Goldmember\"?'"
      ],
      "metadata": {
        "id": "6rABWPD4Z6Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file.close()"
      ],
      "metadata": {
        "id": "rrm4xon_Z8yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = predictor_ner.predict(\n",
        "    sentence=data\n",
        ")"
      ],
      "metadata": {
        "id": "vqXqRFo0AODV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for word, tag in zip(results['words'], results['tags']):\n",
        "#   print(word + \": \" + tag)"
      ],
      "metadata": {
        "id": "JsPKVE2CAcdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results"
      ],
      "metadata": {
        "id": "P9qW3vOHCruq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = []\n",
        "\n",
        "for word, tag in zip(results['words'], results['tags']):\n",
        "  tpl = (word, tag)\n",
        "  lst.append(tpl) "
      ],
      "metadata": {
        "id": "5vk-WsqenE4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_lst = []\n",
        "i = 0\n",
        "n = len(lst)\n",
        "s = \"\"\n",
        "prev_tag = \"\"\n",
        "while i < n:\n",
        "  ps = lst[i][0]\n",
        "  pt = lst[i][1]\n",
        "\n",
        "  if pt[0] == 'B':\n",
        "    if s != \"\":\n",
        "      tpl = (s, prev_tag)\n",
        "      final_lst.append(tpl)\n",
        "    s = ps\n",
        "    prev_tag = pt[2:]\n",
        "\n",
        "  elif pt[0] == 'I': \n",
        "    s = s + \" \" + ps\n",
        "    prev_tag = pt[2:]\n",
        "\n",
        "  elif pt[0] == 'L':\n",
        "    s = s + \" \" + ps\n",
        "    tpl = (s, pt[2:])\n",
        "    final_lst.append(tpl)\n",
        "    s=\"\"\n",
        "\n",
        "  elif pt[0] == 'U':\n",
        "    if s!=\"\":\n",
        "      tpl = (s, prev_tag)\n",
        "      final_lst.append(tpl)\n",
        "    tpl1 = (ps, pt[2:])\n",
        "    final_lst.append(tpl1)\n",
        "    s=\"\"\n",
        "    prev_tag=\"\"\n",
        "\n",
        "  elif pt == \"O\":\n",
        "    if(prev_tag == \"O\"):\n",
        "      if ps == '.' or ps == ',':\n",
        "        s = s+ps\n",
        "      else:\n",
        "        s = s + \" \" + ps\n",
        "    else:\n",
        "      if s!=\"\":\n",
        "        tpl = (s, prev_tag)\n",
        "        final_lst.append(tpl)\n",
        "      s = ps\n",
        "    prev_tag = pt\n",
        "\n",
        "  i+=1\n",
        "\n",
        "if s!=\"\":\n",
        "  tpl = (s, prev_tag)\n",
        "  final_lst.append(tpl)\n",
        "      \n"
      ],
      "metadata": {
        "id": "W8BLJNUAHtGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_lst)"
      ],
      "metadata": {
        "id": "_HkYtVPDl_gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lst)"
      ],
      "metadata": {
        "id": "i6RxAEh0nf2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"\"\n",
        "\n",
        "for item in final_lst:\n",
        "  if para == \"\":\n",
        "    para = item[0]\n",
        "  else:\n",
        "    para = para + \" \" + item[0]"
      ],
      "metadata": {
        "id": "h678BCI6drbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para"
      ],
      "metadata": {
        "id": "zU1Yw2QkeHIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = open('/content/drive/MyDrive/Reading elective/clusters.txt', 'w')\n",
        "# for item in lst:\n",
        "#   line = \"{} : {}\\n\".format(item[0], item[1])\n",
        "#   f.write(line)\n",
        "\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "ar_a7TOoRiAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = open('/content/drive/MyDrive/Reading elective/ner_tags.txt', 'w')\n",
        "# for item in final_lst:\n",
        "#   line = \"{} : {}\\n\".format(item[0], item[1])\n",
        "#   f.write(line)\n",
        "\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "I8ARy4xSRiI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-Midterm"
      ],
      "metadata": {
        "id": "kYc8jXKjzvtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_str = \"\"\n",
        "final_words = []\n",
        "tag_of_intrest = \"PER\"\n",
        "idxs_of_tags = []"
      ],
      "metadata": {
        "id": "bSU-svO1x0t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for tpl in final_lst:\n",
        "  word = \"\"\n",
        "  if tpl[1] != 'O':\n",
        "    word = tpl[0].replace(' ', '_')\n",
        "    final_str = final_str + \" \" + word\n",
        "  else:\n",
        "    word = tpl[0]\n",
        "    final_str = final_str + \" \" + word\n",
        "  if tpl[1] == tag_of_intrest:\n",
        "    idxs_of_tags.append(i)\n",
        "\n",
        "  final_words.append(word)\n",
        "  i+=1\n",
        "\n",
        "final_str = final_str[1:]"
      ],
      "metadata": {
        "id": "vc7LMM24wIhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_str)\n",
        "print(final_words)\n",
        "print(idxs_of_tags)"
      ],
      "metadata": {
        "id": "L_QAdCaczfwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(idxs_of_tags)"
      ],
      "metadata": {
        "id": "RenXR97qQCYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/Reading elective/w2vpandas.model\")"
      ],
      "metadata": {
        "id": "qrckr8gK-tZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gensim.__version__)"
      ],
      "metadata": {
        "id": "Q-5F1UFo-tPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(model.wv.most_similar('Albert_Einstein', topn=20))"
      ],
      "metadata": {
        "id": "Il-vtBbd-tEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.wv.most_similar('Albert_Einstein', topn=10)"
      ],
      "metadata": {
        "id": "--_jf2To-B1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('Germany', topn=10)"
      ],
      "metadata": {
        "id": "-ivzW-cRASrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('England', topn=10)"
      ],
      "metadata": {
        "id": "t-rxA9kBASvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('Gujarat', topn=10)"
      ],
      "metadata": {
        "id": "aabeNcnYYa5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look_up = {'Editis':'Lyon', 'Fritzlar_Air_Base':'Hammerschmidt_Villa'}\n",
        "\n",
        "look_up = {}\n",
        "\n",
        "top_n = 20"
      ],
      "metadata": {
        "id": "pu_WCFT_Q5es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in idxs_of_tags:\n",
        "  lst = []\n",
        "  # print(i)\n",
        "  w = final_words[i]\n",
        "  keys = list(look_up.keys())\n",
        "  values = list(look_up.values())\n",
        "  if w in keys:\n",
        "    # print('b')\n",
        "    final_words[i] = look_up[w]\n",
        "  else:\n",
        "    # print('a')\n",
        "    print(type(model.wv.most_similar(w, topn=top_n)))\n",
        "    lst = model.wv.most_similar(w, topn=top_n)\n",
        "    j=0\n",
        "    while j<top_n:\n",
        "      if lst[i][0] in values:\n",
        "        j+=1\n",
        "        continue\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    if j<top_n:\n",
        "      final_words[i] = lst[j][0]\n",
        "      look_up[w] = lst[j][0]"
      ],
      "metadata": {
        "id": "F_lnz500Q5ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "look_up"
      ],
      "metadata": {
        "id": "pwvWvIhcDTaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(final_words)"
      ],
      "metadata": {
        "id": "7D_rg-7eQ5xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anonymized_string = \"\"\n",
        "\n",
        "for w in final_words:\n",
        "  if anonymized_string == \"\":\n",
        "    anonymized_string = w\n",
        "  else:\n",
        "    anonymized_string = anonymized_string + \" \" + w"
      ],
      "metadata": {
        "id": "ulcubrl8Q544"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(anonymized_string)"
      ],
      "metadata": {
        "id": "BIsGyvJyErxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = anonymized_string.replace('_', ' ')"
      ],
      "metadata": {
        "id": "v66M3F04jTTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "eOK6NMZjlLQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9SE0xwsJ856w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}